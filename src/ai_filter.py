"""AI project filter using LLM"""
import json
import logging
from dataclasses import dataclass
from typing import List
from openai import OpenAI
from src.github_scraper import TrendingProject


logger = logging.getLogger(__name__)


@dataclass
class FilterResult:
    """AI filter result"""
    is_ai_related: bool
    reason: str


class AIFilter:
    """Filter AI-related projects using LLM"""

    AI_KEYWORDS = [
        'ai', 'ml', 'machine learning', 'deep learning', 'neural network',
        'llm', 'gpt', 'transformer', 'bert', 'chatbot', 'computer vision',
        'nlp', 'natural language', 'opencv', 'tensorflow', 'pytorch',
        'stable diffusion', 'gan', 'generative', 'diffusion model',
        'embedding', 'vector database', 'rag', 'agent', 'langchain'
    ]

    def __init__(self, base_url: str, api_key: str, model: str):
        """
        Initialize AI filter

        Args:
            base_url: OpenAI-compatible API base URL
            api_key: API key
            model: Model name
        """
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = model

    def is_ai_related(self, project: TrendingProject) -> FilterResult:
        """
        Determine if project is AI-related

        Args:
            project: Project to check

        Returns:
            FilterResult with is_ai_related flag and reason
        """
        try:
            prompt = f"""åˆ¤æ–­ä»¥ä¸‹GitHubé¡¹ç›®æ˜¯å¦ä¸AIç›¸å…³ï¼ˆæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€LLMã€è®¡ç®—æœºè§†è§‰ã€NLPã€AIå·¥å…·ç­‰ï¼‰ã€‚

é¡¹ç›®åï¼š{project.repo_name}
æè¿°ï¼š{project.description}
è¯­è¨€ï¼š{project.language}

è¯·è¿”å›JSONæ ¼å¼ï¼š{{"is_ai_related": true/false, "reason": "åˆ¤æ–­ç†ç”±"}}"""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIé¡¹ç›®è¯†åˆ«ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=200
            )

            content = response.choices[0].message.content

            # Clean up markdown code blocks if present
            if content.startswith("```"):
                content = content.strip().strip("`")
                if content.startswith("json"):
                    content = content[4:]
                content = content.strip()

            result = json.loads(content)

            return FilterResult(
                is_ai_related=result.get('is_ai_related', False),
                reason=result.get('reason', '')
            )

        except Exception as e:
            logger.warning(f"LLM filter failed for {project.repo_name}, using keyword fallback: {e}")
            # Fallback to keyword matching
            is_ai = self._keyword_fallback(project.description + " " + project.repo_name)
            return FilterResult(
                is_ai_related=is_ai,
                reason="Keyword-based detection (LLM unavailable)"
            )

    def _keyword_fallback(self, text: str) -> bool:
        """Fallback keyword-based detection"""
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.AI_KEYWORDS)

    def batch_filter(self, projects: List[TrendingProject]) -> List[tuple[TrendingProject, FilterResult]]:
        """
        Filter multiple projects

        Args:
            projects: List of projects to filter

        Returns:
            List of (project, result) tuples for AI-related projects
        """
        results = []

        for project in projects:
            result = self.is_ai_related(project)
            if result.is_ai_related:
                results.append((project, result))
                logger.info(f"âœ“ AI project: {project.repo_name} - {result.reason}")
            else:
                logger.debug(f"âœ— Not AI: {project.repo_name}")

        return results

    def generate_daily_summary(self, projects: List[tuple[TrendingProject, FilterResult]]) -> str:
        """
        Generate a summary of the provided projects, highlighting relevance to Sohu's business.

        Args:
            projects: List of (project, filter_result) tuples

        Returns:
            Summary text
        """
        if not projects:
            return ""

        projects_text = ""
        for i, (p, r) in enumerate(projects, 1):
            projects_text += f"{i}. {p.repo_name}: {p.description} (Language: {p.language})\n"

        prompt = f"""
åˆ†æä»¥ä¸‹ä»Šæ—¥GitHubçƒ­é—¨AIé¡¹ç›®åˆ—è¡¨ï¼š

{projects_text}

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. **æ¯æ—¥è¶‹åŠ¿æ€»ç»“**ï¼šç”¨ä¸€æ®µè¯ç®€è¦æ¦‚æ‹¬ä»Šæ—¥é¡¹ç›®çš„æ•´ä½“æŠ€æœ¯æ–¹å‘æˆ–çƒ­ç‚¹ï¼ˆå¦‚Agentã€RAGã€å¤šæ¨¡æ€ç­‰ï¼‰ã€‚
2. **æœç‹ä¸šåŠ¡ä»·å€¼è¯„ä¼°**ï¼š
   è¯·ä»”ç»†è¯„ä¼°è¿™äº›é¡¹ç›®å¯¹æœç‹å…¬å¸çš„**æœç´¢å¼•æ“**ã€**æ¨èç³»ç»Ÿ**ã€**AIåŸºç¡€è®¾æ–½**ï¼ˆè®­ç»ƒ/æ¨ç†/éƒ¨ç½²ï¼‰æ˜¯å¦æœ‰æ˜ç¡®çš„æŠ€æœ¯ä»·å€¼æˆ–åº”ç”¨æ½œåŠ›ã€‚
   - åªæœ‰åœ¨ç¡®å®ç›¸å…³ä¸”æœ‰æå‡å¯èƒ½æ—¶æ‰æåŠã€‚
   - å¦‚æœæœ‰ï¼Œè¯·åˆ—å‡ºé¡¹ç›®åå¹¶è¯¦ç»†è¯´æ˜å…¶å¯¹ç‰¹å®šä¸šåŠ¡åœºæ™¯ï¼ˆå¦‚æœç´¢ç›¸å…³æ€§ã€æ¨èå¤šæ ·æ€§ã€æ¨¡å‹æ¨ç†åŠ é€Ÿç­‰ï¼‰çš„æ½œåœ¨æ”¶ç›Šã€‚
   - å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„ç›´æ¥ä»·å€¼ï¼Œåˆ™ä¸è¦ç¼–é€ ï¼Œè¿™éƒ¨åˆ†ç•™ç©ºå³å¯ã€‚

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨Markdownæ ¼å¼ã€‚
- æ€»ç»“éƒ¨åˆ†å°½é‡ç²¾ç‚¼ã€‚
- ä¸šåŠ¡è¯„ä¼°éƒ¨åˆ†å¦‚æœæœ‰å†…å®¹ï¼Œè¯·ä½¿ç”¨"ğŸš€ **æœç‹ä¸šåŠ¡ä»·å€¼åˆ†æ**"ä½œä¸ºæ ‡é¢˜ã€‚
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æŠ€æœ¯ä¸“å®¶ï¼Œæ“…é•¿è¯„ä¼°å¼€æºé¡¹ç›®å¯¹ä¼ä¸šä¸šåŠ¡çš„ä»·å€¼ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4,
                max_tokens=800
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Failed to generate summary: {e}")
            return "ï¼ˆç”Ÿæˆæ€»ç»“å¤±è´¥ï¼‰"
